# SPDX-License-Identifier: GPL-3.0-or-later
#
# Toolify: Empower any LLM with function calling capabilities and token counting.
# Copyright (C) 2025 FunnyCups (https://github.com/funnycups)

"""
ç»Ÿä¸€æœåŠ¡: 
1. å‡½æ•°è°ƒç”¨å¢å¼ºä»£ç†æœåŠ¡ - ç»™ä»»ä½•LLMæ·»åŠ å‡½æ•°è°ƒç”¨åŠŸèƒ½
2. Tokenè®¡æ•°ä»£ç†æœåŠ¡ - ç²¾ç¡®è®¡ç®—å’Œç›‘æ§tokenä½¿ç”¨æƒ…å†µ
"""

import os
import re
import json
import uuid
import httpx
import secrets
import string
import traceback
import time
import random
import threading
import logging
import tiktoken
import uvicorn
from typing import List, Dict, Any, Optional, Literal, Union, AsyncGenerator
from collections import OrderedDict
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request, Header, HTTPException, Depends
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field, ValidationError, validator, field_validator
from openai import AsyncOpenAI

from config_loader import config_loader

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ==================== Token è®¡ç®—å™¨ ====================
class TokenCounter:
    """ä½¿ç”¨ tiktoken è®¡ç®— token æ•°é‡"""
    
    def __init__(self):
        self.encoders = {}
    
    def get_encoder(self, model: str):
        """è·å–æˆ–åˆ›å»ºå¯¹åº”æ¨¡å‹çš„ç¼–ç å™¨"""
        if model not in self.encoders:
            try:
                # å°è¯•è·å–æ¨¡å‹ä¸“ç”¨ç¼–ç å™¨
                self.encoders[model] = tiktoken.encoding_for_model(model)
            except KeyError:
                # é»˜è®¤ä½¿ç”¨
                logger.warning(f"Model {model} not found, using o200k_base encoding")
                self.encoders[model] = tiktoken.get_encoding("o200k_base")
        return self.encoders[model]
    
    def count_tokens(self, messages: list, model: str = "gpt-5-high") -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°é‡"""
        encoder = self.get_encoder(model)
        
        # æ ¹æ®æ¨¡å‹é€‰æ‹©è®¡ç®—æ–¹å¼
        if model.startswith("gpt-5-high") or model.startswith("gpt-4"):
            return self._count_chat_tokens(messages, encoder, model)
        else:
            # ç®€å•è®¡ç®—ï¼šæ‹¼æ¥æ‰€æœ‰æ–‡æœ¬å†…å®¹
            text_content = []
            for msg in messages:
                content = msg.get("content", "")
                # å¤„ç†contentå¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ(å¤šæ¨¡æ€æ¶ˆæ¯)
                if isinstance(content, list):
                    # åªæå–åˆ—è¡¨ä¸­çš„æ–‡æœ¬éƒ¨åˆ†
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            text_content.append(item.get("text", ""))
                elif isinstance(content, str):
                    text_content.append(content)
            
            # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬
            full_text = " ".join(text_content)
            return len(encoder.encode(full_text))
    
    def _count_chat_tokens(self, messages: list, encoder, model: str) -> int:
        """Chat æ¨¡å‹çš„ç²¾ç¡® token è®¡ç®—"""
        tokens_per_message = 3  # æ¯æ¡æ¶ˆæ¯çš„å›ºå®šå¼€é”€
        tokens_per_name = 1      # å¦‚æœæœ‰ name å­—æ®µ
        
        num_tokens = 0
        for message in messages:
            num_tokens += tokens_per_message
            for key, value in message.items():
                if key == "content":
                    # å¤„ç†contentå¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ(å¤šæ¨¡æ€æ¶ˆæ¯)
                    if isinstance(value, list):
                        for item in value:
                            if isinstance(item, dict) and item.get("type") == "text":
                                content_text = item.get("text", "")
                                num_tokens += len(encoder.encode(content_text))
                    elif isinstance(value, str):
                        num_tokens += len(encoder.encode(value))
                elif key == "name":
                    num_tokens += tokens_per_name
                    if isinstance(value, str):
                        num_tokens += len(encoder.encode(value))
                elif isinstance(value, str):
                    num_tokens += len(encoder.encode(value))
        
        num_tokens += 3  # æ¯æ¬¡å›å¤éƒ½æœ‰å›ºå®šçš„ priming
        return num_tokens
    
    def count_text_tokens(self, text: str, model: str = "gpt-5-high") -> int:
        """è®¡ç®—çº¯æ–‡æœ¬çš„ token æ•°é‡"""
        encoder = self.get_encoder(model)
        return len(encoder.encode(text))

# å…¨å±€ token è®¡æ•°å™¨
token_counter = TokenCounter()

# ==================== å·¥å…·è°ƒç”¨æ˜ å°„ç®¡ç†å™¨ ====================
class ToolCallMappingManager:
    """
    å·¥å…·è°ƒç”¨æ˜ å°„ç®¡ç†å™¨ï¼Œå…·æœ‰TTLï¼ˆç”Ÿå­˜æ—¶é—´ï¼‰å’Œå¤§å°é™åˆ¶
    
    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨è¿‡æœŸæ¸…ç† - æ¡ç›®åœ¨æŒ‡å®šæ—¶é—´åè‡ªåŠ¨åˆ é™¤
    2. å¤§å°é™åˆ¶ - é˜²æ­¢æ— é™å†…å­˜å¢é•¿
    3. LRUæ·˜æ±° - åœ¨è¾¾åˆ°å¤§å°é™åˆ¶æ—¶åˆ é™¤æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„æ¡ç›®
    4. çº¿ç¨‹å®‰å…¨ - æ”¯æŒå¹¶å‘è®¿é—®
    5. å®šæœŸæ¸…ç† - åå°çº¿ç¨‹å®šæœŸæ¸…ç†è¿‡æœŸæ¡ç›®
    """
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600, cleanup_interval: int = 300):
        """
        åˆå§‹åŒ–æ˜ å°„ç®¡ç†å™¨
        
        å‚æ•°ï¼š
            max_size: å­˜å‚¨æ¡ç›®çš„æœ€å¤§æ•°é‡
            ttl_seconds: æ¡ç›®ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
            cleanup_interval: æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
        """
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cleanup_interval = cleanup_interval
        
        self._data: OrderedDict[str, Dict[str, Any]] = OrderedDict()
        self._timestamps: Dict[str, float] = {}
        self._lock = threading.RLock()
        
        self._cleanup_thread = threading.Thread(target=self._periodic_cleanup, daemon=True)
        self._cleanup_thread.start()
        
        logger.debug(f"ğŸ”§ [INIT] å·¥å…·è°ƒç”¨æ˜ å°„ç®¡ç†å™¨å·²å¯åŠ¨ - æœ€å¤§æ¡ç›®: {max_size}, TTL: {ttl_seconds}ç§’, æ¸…ç†é—´éš”: {cleanup_interval}ç§’")
    
    def store(self, tool_call_id: str, name: str, args: dict, description: str = "") -> None:
        """å­˜å‚¨å·¥å…·è°ƒç”¨æ˜ å°„"""
        with self._lock:
            current_time = time.time()
            
            if tool_call_id in self._data:
                del self._data[tool_call_id]
                del self._timestamps[tool_call_id]
            
            while len(self._data) >= self.max_size:
                oldest_key = next(iter(self._data))
                del self._data[oldest_key]
                del self._timestamps[oldest_key]
                logger.debug(f"ğŸ”§ [CLEANUP] å› å¤§å°é™åˆ¶åˆ é™¤äº†æœ€æ—§æ¡ç›®: {oldest_key}")
            
            self._data[tool_call_id] = {
                "name": name,
                "args": args,
                "description": description,
                "created_at": current_time
            }
            self._timestamps[tool_call_id] = current_time
            
            logger.debug(f"ğŸ”§ å­˜å‚¨äº†å·¥å…·è°ƒç”¨æ˜ å°„: {tool_call_id} -> {name}")
            logger.debug(f"ğŸ”§ å½“å‰æ˜ å°„è¡¨å¤§å°: {len(self._data)}")
    
    def get(self, tool_call_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å·¥å…·è°ƒç”¨æ˜ å°„ï¼ˆæ›´æ–°LRUé¡ºåºï¼‰"""
        with self._lock:
            current_time = time.time()
            
            if tool_call_id not in self._data:
                logger.debug(f"ğŸ”§ æœªæ‰¾åˆ°å·¥å…·è°ƒç”¨æ˜ å°„: {tool_call_id}")
                logger.debug(f"ğŸ”§ å½“å‰æ˜ å°„è¡¨ä¸­çš„æ‰€æœ‰ID: {list(self._data.keys())}")
                return None
            
            if current_time - self._timestamps[tool_call_id] > self.ttl_seconds:
                logger.debug(f"ğŸ”§ å·¥å…·è°ƒç”¨æ˜ å°„å·²è¿‡æœŸ: {tool_call_id}")
                del self._data[tool_call_id]
                del self._timestamps[tool_call_id]
                return None
            
            result = self._data[tool_call_id]
            self._data.move_to_end(tool_call_id)
            
            logger.debug(f"ğŸ”§ æ‰¾åˆ°å·¥å…·è°ƒç”¨æ˜ å°„: {tool_call_id} -> {result['name']}")
            return result
    
    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸæ¡ç›®ï¼Œè¿”å›æ¸…ç†çš„æ¡ç›®æ•°é‡"""
        with self._lock:
            current_time = time.time()
            expired_keys = []
            
            for key, timestamp in self._timestamps.items():
                if current_time - timestamp > self.ttl_seconds:
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self._data[key]
                del self._timestamps[key]
            
            if expired_keys:
                logger.debug(f"ğŸ”§ [CLEANUP] å·²æ¸…ç† {len(expired_keys)} ä¸ªè¿‡æœŸæ¡ç›®")
            
            return len(expired_keys)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            current_time = time.time()
            expired_count = sum(1 for ts in self._timestamps.values()
                               if current_time - ts > self.ttl_seconds)
            
            return {
                "total_entries": len(self._data),
                "expired_entries": expired_count,
                "active_entries": len(self._data) - expired_count,
                "max_size": self.max_size,
                "ttl_seconds": self.ttl_seconds,
                "memory_usage_ratio": len(self._data) / self.max_size
            }
    
    def _periodic_cleanup(self) -> None:
        """åå°å®šæœŸæ¸…ç†çº¿ç¨‹"""
        while True:
            try:
                time.sleep(self.cleanup_interval)
                cleaned = self.cleanup_expired()
                
                stats = self.get_stats()
                if stats["total_entries"] > 0:
                    logger.debug(f"ğŸ”§ [STATS] æ˜ å°„è¡¨çŠ¶æ€: æ€»è®¡={stats['total_entries']}, "
                               f"æ´»åŠ¨={stats['active_entries']}, å†…å­˜ä½¿ç”¨ç‡={stats['memory_usage_ratio']:.1%}")
                
            except Exception as e:
                logger.error(f"âŒ åå°æ¸…ç†çº¿ç¨‹å¼‚å¸¸: {e}")

# ==================== è¾…åŠ©å‡½æ•° ====================
def generate_random_trigger_signal() -> str:
    """ç”Ÿæˆä¸€ä¸ªéšæœºçš„ã€è‡ªé—­åˆçš„è§¦å‘ä¿¡å·ï¼Œå¦‚ <Function_AB1c_Start/>"""
    chars = string.ascii_letters + string.digits
    random_str = ''.join(secrets.choice(chars) for _ in range(4))
    return f"<Function_{random_str}_Start/>"

# ==================== é…ç½®åŠ è½½ ====================
try:
    # åŠ è½½é…ç½®æ–‡ä»¶
    app_config = config_loader.load_config()
    
    log_level_str = app_config.features.log_level
    if log_level_str == "DISABLED":
        log_level = logging.CRITICAL + 1
    else:
        log_level = getattr(logging, log_level_str, logging.INFO)
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    logger.info(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {config_loader.config_path}")
    logger.info(f"ğŸ“Š å·²é…ç½® {len(app_config.upstream_services)} ä¸ªä¸Šæ¸¸æœåŠ¡")
    logger.info(f"ğŸ”‘ å·²é…ç½® {len(app_config.client_authentication.allowed_keys)} ä¸ªå®¢æˆ·ç«¯å¯†é’¥")
    
    MODEL_TO_SERVICE_MAPPING, ALIAS_MAPPING = config_loader.get_model_to_service_mapping()
    DEFAULT_SERVICE = config_loader.get_default_service()
    ALLOWED_CLIENT_KEYS = config_loader.get_allowed_client_keys()
    GLOBAL_TRIGGER_SIGNAL = generate_random_trigger_signal()
    
    logger.info(f"ğŸ¯ å·²é…ç½® {len(MODEL_TO_SERVICE_MAPPING)} ä¸ªæ¨¡å‹æ˜ å°„")
    if ALIAS_MAPPING:
        logger.info(f"ğŸ”„ å·²é…ç½® {len(ALIAS_MAPPING)} ä¸ªæ¨¡å‹åˆ«å: {list(ALIAS_MAPPING.keys())}")
    logger.info(f"ğŸ”„ é»˜è®¤æœåŠ¡: {DEFAULT_SERVICE['name']}")
    
except Exception as e:
    logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {type(e).__name__}")
    logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {str(e)}")
    logger.error("ğŸ’¡ è¯·ç¡®ä¿config.yamlæ–‡ä»¶å­˜åœ¨å¹¶ä¸”æ ¼å¼æ­£ç¡®")
    
    # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®Tokenè®¡æ•°å™¨ä»£ç†
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "sk-123456")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "http://127.0.0.1:5108/v1")
    PROXY_PORT = int(os.getenv("PROXY_PORT", "5112"))
    PROXY_HOST = os.getenv("PROXY_HOST", "0.0.0.0")
    logger.info(f"âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®Tokenè®¡æ•°å™¨ä»£ç†")
    logger.info(f"âœ… OpenAIåŸºç¡€URL: {OPENAI_BASE_URL}")

# ==================== æ˜ å°„å‡½æ•° ====================
def store_tool_call_mapping(tool_call_id: str, name: str, args: dict, description: str = ""):
    """å­˜å‚¨å·¥å…·è°ƒç”¨IDä¸è°ƒç”¨å†…å®¹ä¹‹é—´çš„æ˜ å°„"""
    TOOL_CALL_MAPPING_MANAGER.store(tool_call_id, name, args, description)

def get_tool_call_mapping(tool_call_id: str) -> Optional[Dict[str, Any]]:
    """è·å–å¯¹åº”äºå·¥å…·è°ƒç”¨IDçš„è°ƒç”¨å†…å®¹"""
    return TOOL_CALL_MAPPING_MANAGER.get(tool_call_id)

def format_tool_result_for_ai(tool_call_id: str, result_content: str) -> str:
    """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨ç»“æœï¼Œä½¿AIèƒ½å¤Ÿç†è§£ï¼Œä½¿ç”¨è‹±æ–‡æç¤ºå’ŒXMLç»“æ„"""
    logger.debug(f"ğŸ”§ æ ¼å¼åŒ–å·¥å…·è°ƒç”¨ç»“æœ: tool_call_id={tool_call_id}")
    tool_info = get_tool_call_mapping(tool_call_id)
    if not tool_info:
        logger.debug(f"ğŸ”§ æœªæ‰¾åˆ°å·¥å…·è°ƒç”¨æ˜ å°„ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼")
        return f"Tool execution result:\n<tool_result>\n{result_content}\n</tool_result>"
    
    formatted_text = f"""Tool execution result:
- Tool name: {tool_info['name']}
- Execution result:
<tool_result>
{result_content}
</tool_result>"""
    
    logger.debug(f"ğŸ”§ æ ¼å¼åŒ–å®Œæˆï¼Œå·¥å…·åç§°: {tool_info['name']}")
    return formatted_text

def format_assistant_tool_calls_for_ai(tool_calls: List[Dict[str, Any]], trigger_signal: str) -> str:
    """å°†åŠ©æ‰‹å·¥å…·è°ƒç”¨æ ¼å¼åŒ–ä¸ºAIå¯è¯»çš„å­—ç¬¦ä¸²æ ¼å¼"""
    logger.debug(f"ğŸ”§ æ ¼å¼åŒ–åŠ©æ‰‹å·¥å…·è°ƒç”¨. æ•°é‡: {len(tool_calls)}")
    
    xml_calls_parts = []
    for tool_call in tool_calls:
        function_info = tool_call.get("function", {})
        name = function_info.get("name", "")
        arguments_json = function_info.get("arguments", "{}")
        
        try:
            # é¦–å…ˆå°è¯•ä½œä¸ºJSONåŠ è½½ã€‚å¦‚æœæ˜¯æœ‰æ•ˆçš„JSONå­—ç¬¦ä¸²ï¼Œåˆ™è§£æå®ƒã€‚
            args_dict = json.loads(arguments_json)
        except (json.JSONDecodeError, TypeError):
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„JSONå­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è§†ä¸ºç®€å•å­—ç¬¦ä¸²ã€‚
            args_dict = {"raw_arguments": arguments_json}

        args_parts = []
        for key, value in args_dict.items():
            # å°†å€¼è½¬æ¢å›JSONå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿åœ¨XMLå†…éƒ¨ä¸€è‡´è¡¨ç¤ºã€‚
            json_value = json.dumps(value, ensure_ascii=False)
            args_parts.append(f"<{key}>{json_value}</{key}>")
        
        args_content = "\n".join(args_parts)
        
        xml_call = f"<function_call>\n<tool>{name}</tool>\n<args>\n{args_content}\n</args>\n</function_call>"
        xml_calls_parts.append(xml_call)

    all_calls = "\n".join(xml_calls_parts)
    final_str = f"{trigger_signal}\n<function_calls>\n{all_calls}\n</function_calls>"
    
    logger.debug("ğŸ”§ åŠ©æ‰‹å·¥å…·è°ƒç”¨æ ¼å¼åŒ–æˆåŠŸã€‚")
    return final_str

def get_function_call_prompt_template(trigger_signal: str) -> str:
    """
    åŸºäºåŠ¨æ€è§¦å‘ä¿¡å·ç”Ÿæˆæç¤ºæ¨¡æ¿
    """
    custom_template = app_config.features.prompt_template
    if custom_template:
        logger.info("ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„è‡ªå®šä¹‰æç¤ºæ¨¡æ¿")
        return custom_template.format(
            trigger_signal=trigger_signal,
            tools_list="{tools_list}"
        )
    
    return f"""
æ‚¨å¯ä»¥è®¿é—®ä»¥ä¸‹å¯ç”¨å·¥å…·æ¥å¸®åŠ©è§£å†³é—®é¢˜:

{{tools_list}}

**é‡è¦ä¸Šä¸‹æ–‡è¯´æ˜:**
1. æ‚¨å¯ä»¥åœ¨å•ä¸ªå“åº”ä¸­è°ƒç”¨å¤šä¸ªå·¥å…·ã€‚
2. å¯¹è¯ä¸Šä¸‹æ–‡ä¸­å¯èƒ½å·²ç»åŒ…å«äº†æ¥è‡ªå…ˆå‰å‡½æ•°è°ƒç”¨çš„å·¥å…·æ‰§è¡Œç»“æœã€‚ä»”ç»†æŸ¥çœ‹å¯¹è¯å†å²ï¼Œä»¥é¿å…ä¸å¿…è¦çš„é‡å¤å·¥å…·è°ƒç”¨ã€‚
3. å½“ä¸Šä¸‹æ–‡ä¸­å­˜åœ¨å·¥å…·æ‰§è¡Œç»“æœæ—¶ï¼Œå®ƒä»¬å°†ä½¿ç”¨<tool_result>...</tool_result>ç­‰XMLæ ‡ç­¾è¿›è¡Œæ ¼å¼åŒ–ï¼Œä»¥ä¾¿äºè¯†åˆ«ã€‚
4. è¿™æ˜¯å”¯ä¸€å¯ä»¥ç”¨äºå·¥å…·è°ƒç”¨çš„æ ¼å¼ï¼Œä»»ä½•åå·®éƒ½å°†å¯¼è‡´å¤±è´¥ã€‚

å½“æ‚¨éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œå¿…é¡»ä¸¥æ ¼éµå¾ªæ­¤æ ¼å¼ã€‚è¯·å‹¿åœ¨å·¥å…·è°ƒç”¨è¯­æ³•çš„ç¬¬ä¸€è¡Œå’Œç¬¬äºŒè¡Œæ·»åŠ ä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€è§£é‡Šæˆ–å¯¹è¯:

1. å¼€å§‹å·¥å…·è°ƒç”¨æ—¶ï¼Œåœ¨æ–°è¡Œä¸Šå‡†ç¡®è¾“å…¥:
{trigger_signal}
å‰åæ²¡æœ‰ç©ºæ ¼ï¼Œå®Œå…¨æŒ‰ç…§ä¸Šé¢æ‰€ç¤ºè¾“å‡ºã€‚è§¦å‘ä¿¡å·å¿…é¡»ç‹¬è‡ªå ä¸€è¡Œï¼Œä¸”åªå‡ºç°ä¸€æ¬¡ã€‚

2. ä»ç¬¬äºŒè¡Œå¼€å§‹ï¼Œç«‹å³è·Ÿä¸Šå®Œæ•´çš„<function_calls> XMLå—ã€‚

3. å¯¹äºå¤šä¸ªå·¥å…·è°ƒç”¨ï¼Œåœ¨åŒä¸€ä¸ª<function_calls>åŒ…è£…å™¨ä¸­åŒ…å«å¤šä¸ª<function_call>å—ã€‚

4. ä¸è¦åœ¨ç»“æŸçš„</function_calls>æ ‡ç­¾åæ·»åŠ ä»»ä½•æ–‡æœ¬æˆ–è§£é‡Šã€‚

ä¸¥æ ¼çš„å‚æ•°é”®è§„åˆ™:
- æ‚¨å¿…é¡»å®Œå…¨æŒ‰ç…§å®šä¹‰ä½¿ç”¨å‚æ•°é”®ï¼ˆåŒºåˆ†å¤§å°å†™å’Œæ ‡ç‚¹ç¬¦å·ï¼‰ã€‚ä¸è¦é‡å‘½åã€æ·»åŠ æˆ–åˆ é™¤å­—ç¬¦ã€‚
- å¦‚æœé”®ä»¥è¿å­—ç¬¦å¼€å¤´ï¼ˆä¾‹å¦‚-iã€-Cï¼‰ï¼Œæ‚¨å¿…é¡»åœ¨æ ‡ç­¾åç§°ä¸­ä¿ç•™è¿å­—ç¬¦ã€‚ä¾‹å¦‚ï¼š<-i>true</-i>ï¼Œ<-C>2</-C>ã€‚
- åˆ‡å‹¿å°†"-i"è½¬æ¢ä¸º"i"æˆ–å°†"-C"è½¬æ¢ä¸º"C"ã€‚ä¸è¦å°†å‚æ•°é”®å¤æ•°åŒ–ã€ç¿»è¯‘æˆ–åˆ«åã€‚
- <tool>æ ‡ç­¾å¿…é¡»åŒ…å«åˆ—è¡¨ä¸­å·¥å…·çš„ç¡®åˆ‡åç§°ã€‚ä»»ä½•å…¶ä»–å·¥å…·åç§°éƒ½æ˜¯æ— æ•ˆçš„ã€‚
- <args>å¿…é¡»åŒ…å«è¯¥å·¥å…·çš„æ‰€æœ‰å¿…éœ€å‚æ•°ã€‚

æ­£ç¡®ç¤ºä¾‹ï¼ˆå¤šä¸ªå·¥å…·è°ƒç”¨ï¼ŒåŒ…æ‹¬è¿å­—ç¬¦é”®ï¼‰:
...å“åº”å†…å®¹ï¼ˆå¯é€‰ï¼‰...
{trigger_signal}
<function_calls>
    <function_call>
        <tool>Grep</tool>
        <args>
            <-i>true</-i>
            <-C>2</-C>
            <path>.</path>
        </args>
    </function_call>
    <function_call>
        <tool>search</tool>
        <args>
            <keywords>["Python Document", "how to use python"]</keywords>
        </args>
    </function_call>
  </function_calls>

é”™è¯¯ç¤ºä¾‹ï¼ˆé¢å¤–æ–‡æœ¬+é”™è¯¯çš„é”®åç§° - ä¸è¦è¿™æ ·åšï¼‰:
...å“åº”å†…å®¹ï¼ˆå¯é€‰ï¼‰...
{trigger_signal}
æˆ‘å°†ä¸ºæ‚¨è°ƒç”¨å·¥å…·ã€‚
<function_calls>
    <function_call>
        <tool>Grep</tool>
        <args>
            <i>true</i>
            <C>2</C>
            <path>.</path>
        </args>
    </function_call>
</function_calls>

ç°åœ¨è¯·å‡†å¤‡ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„èŒƒæ“ä½œã€‚
"""

def remove_think_blocks(text: str) -> str:
    """
    æš‚æ—¶åˆ é™¤æ‰€æœ‰<think>...</think>å—ä»¥ä¾¿XMLè§£æ
    æ”¯æŒåµŒå¥—thinkæ ‡ç­¾
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä»…ç”¨äºä¸´æ—¶è§£æï¼Œä¸å½±å“è¿”å›ç»™ç”¨æˆ·çš„åŸå§‹å†…å®¹
    """
    while '<think>' in text and '</think>' in text:
        start_pos = text.find('<think>')
        if start_pos == -1:
            break
        
        pos = start_pos + 7
        depth = 1
        
        while pos < len(text) and depth > 0:
            if text[pos:pos+7] == '<think>':
                depth += 1
                pos += 7
            elif text[pos:pos+8] == '</think>':
                depth -= 1
                pos += 8
            else:
                pos += 1
        
        if depth == 0:
            text = text[:start_pos] + text[pos:]
        else:
            break
    
    return text

def parse_function_calls_xml(xml_string: str, trigger_signal: str) -> Optional[List[Dict[str, Any]]]:
    """
    å¢å¼ºçš„XMLè§£æå‡½æ•°ï¼Œæ”¯æŒåŠ¨æ€è§¦å‘ä¿¡å·
    1. ä¿ç•™<think>...</think>å—ï¼ˆåº”è¯¥æ­£å¸¸è¿”å›ç»™ç”¨æˆ·ï¼‰
    2. ä»…åœ¨è§£æfunction_callsæ—¶æš‚æ—¶åˆ é™¤thinkå—ï¼Œä»¥é˜²æ­¢thinkå†…å®¹å¹²æ‰°XMLè§£æ
    3. æŸ¥æ‰¾è§¦å‘ä¿¡å·çš„æœ€åä¸€æ¬¡å‡ºç°
    4. ä»æœ€åä¸€ä¸ªè§¦å‘ä¿¡å·å¼€å§‹è§£æfunction_calls
    """
    logger.debug(f"ğŸ”§ æ”¹è¿›çš„è§£æå™¨å¼€å§‹å¤„ç†ï¼Œè¾“å…¥é•¿åº¦: {len(xml_string) if xml_string else 0}")
    logger.debug(f"ğŸ”§ ä½¿ç”¨è§¦å‘ä¿¡å·: {trigger_signal[:20]}...")
    
    if not xml_string or trigger_signal not in xml_string:
        logger.debug(f"ğŸ”§ è¾“å…¥ä¸ºç©ºæˆ–ä¸åŒ…å«è§¦å‘ä¿¡å·")
        return None
    
    cleaned_content = remove_think_blocks(xml_string)
    logger.debug(f"ğŸ”§ ä¸´æ—¶åˆ é™¤thinkå—åçš„å†…å®¹é•¿åº¦: {len(cleaned_content)}")
    
    signal_positions = []
    start_pos = 0
    while True:
        pos = cleaned_content.find(trigger_signal, start_pos)
        if pos == -1:
            break
        signal_positions.append(pos)
        start_pos = pos + 1
    
    if not signal_positions:
        logger.debug(f"ğŸ”§ åœ¨æ¸…ç†åçš„å†…å®¹ä¸­æœªæ‰¾åˆ°è§¦å‘ä¿¡å·")
        return None
    
    logger.debug(f"ğŸ”§ æ‰¾åˆ° {len(signal_positions)} ä¸ªè§¦å‘ä¿¡å·ä½ç½®: {signal_positions}")
    
    last_signal_pos = signal_positions[-1]
    content_after_signal = cleaned_content[last_signal_pos:]
    logger.debug(f"ğŸ”§ ä»æœ€åä¸€ä¸ªè§¦å‘ä¿¡å·å¼€å§‹çš„å†…å®¹: {repr(content_after_signal[:100])}")
    
    calls_content_match = re.search(r"<function_calls>([\s\S]*?)</function_calls>", content_after_signal)
    if not calls_content_match:
        logger.debug(f"ğŸ”§ æœªæ‰¾åˆ°function_callsæ ‡ç­¾")
        return None
    
    calls_content = calls_content_match.group(1)
    logger.debug(f"ğŸ”§ function_callså†…å®¹: {repr(calls_content)}")
    
    results = []
    call_blocks = re.findall(r"<function_call>([\s\S]*?)</function_call>", calls_content)
    logger.debug(f"ğŸ”§ æ‰¾åˆ° {len(call_blocks)} ä¸ªfunction_callå—")
    
    for i, block in enumerate(call_blocks):
        logger.debug(f"ğŸ”§ å¤„ç†function_call #{i+1}: {repr(block)}")
        
        tool_match = re.search(r"<tool>(.*?)</tool>", block)
        if not tool_match:
            logger.debug(f"ğŸ”§ åœ¨å— #{i+1} ä¸­æœªæ‰¾åˆ°toolæ ‡ç­¾")
            continue
        
        name = tool_match.group(1).strip()
        args = {}
        
        args_block_match = re.search(r"<args>([\s\S]*?)</args>", block)
        if args_block_match:
            args_content = args_block_match.group(1)
            # æ”¯æŒåŒ…å«è¿å­—ç¬¦çš„å‚æ•°æ ‡ç­¾åç§°ï¼ˆå¦‚-iã€-Aï¼‰ï¼›åŒ¹é…ä»»ä½•éç©ºæ ¼ã€é'>'å’Œé'/'å­—ç¬¦
            arg_matches = re.findall(r"<([^\s>/]+)>([\s\S]*?)</\1>", args_content)

            def _coerce_value(v: str):
                try:
                    return json.loads(v)
                except Exception:
                    pass
                return v

            for k, v in arg_matches:
                args[k] = _coerce_value(v)
        
        result = {"name": name, "args": args}
        results.append(result)
        logger.debug(f"ğŸ”§ æ·»åŠ äº†å·¥å…·è°ƒç”¨: {result}")
    
    logger.debug(f"ğŸ”§ æœ€ç»ˆè§£æç»“æœ: {results}")
    return results if results else None

class StreamingFunctionCallDetector:
    """å¢å¼ºçš„æµå¼å‡½æ•°è°ƒç”¨æ£€æµ‹å™¨ï¼Œæ”¯æŒåŠ¨æ€è§¦å‘ä¿¡å·ï¼Œé¿å…åœ¨<think>æ ‡ç­¾å†…è¯¯åˆ¤
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. é¿å…åœ¨<think>å—å†…è§¦å‘å·¥å…·è°ƒç”¨æ£€æµ‹
    2. æ­£å¸¸å°†<think>å—å†…å®¹è¾“å‡ºç»™ç”¨æˆ·
    3. æ”¯æŒåµŒå¥—thinkæ ‡ç­¾
    """
    
    def __init__(self, trigger_signal: str):
        self.trigger_signal = trigger_signal
        self.reset()
    
    def reset(self):
        self.content_buffer = ""
        self.state = "detecting"  # detecting, tool_parsing
        self.in_think_block = False
        self.think_depth = 0
        self.signal = self.trigger_signal
        self.signal_len = len(self.signal)
    
    def process_chunk(self, delta_content: str) -> tuple[bool, str]:
        """
        å¤„ç†æµå¼å†…å®¹å—
        è¿”å›: (æ˜¯å¦æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨, è¦è¾“å‡ºçš„å†…å®¹)
        """
        if not delta_content:
            return False, ""
        
        self.content_buffer += delta_content
        content_to_yield = ""
        
        if self.state == "tool_parsing":
            return False, ""
        
        if delta_content:
            logger.debug(f"ğŸ”§ å¤„ç†å—: {repr(delta_content[:50])}{'...' if len(delta_content) > 50 else ''}, ç¼“å†²åŒºé•¿åº¦: {len(self.content_buffer)}, thinkçŠ¶æ€: {self.in_think_block}")
        
        i = 0
        while i < len(self.content_buffer):
            skip_chars = self._update_think_state(i)
            if skip_chars > 0:
                for j in range(skip_chars):
                    if i + j < len(self.content_buffer):
                        content_to_yield += self.content_buffer[i + j]
                i += skip_chars
                continue
            
            if not self.in_think_block and self._can_detect_signal_at(i):
                if self.content_buffer[i:i+self.signal_len] == self.signal:
                    logger.debug(f"ğŸ”§ æ”¹è¿›çš„æ£€æµ‹å™¨: åœ¨éthinkå—ä¸­æ£€æµ‹åˆ°è§¦å‘ä¿¡å·! ä¿¡å·: {self.signal[:20]}...")
                    logger.debug(f"ğŸ”§ è§¦å‘ä¿¡å·ä½ç½®: {i}, thinkçŠ¶æ€: {self.in_think_block}, thinkæ·±åº¦: {self.think_depth}")
                    self.state = "tool_parsing"
                    self.content_buffer = self.content_buffer[i:]
                    return True, content_to_yield
            
            remaining_len = len(self.content_buffer) - i
            if remaining_len < self.signal_len or remaining_len < 8:
                break
            
            content_to_yield += self.content_buffer[i]
            i += 1
        
        self.content_buffer = self.content_buffer[i:]
        return False, content_to_yield
    
    def _update_think_state(self, pos: int):
        """æ›´æ–°thinkæ ‡ç­¾çŠ¶æ€ï¼Œæ”¯æŒåµŒå¥—"""
        remaining = self.content_buffer[pos:]
        
        if remaining.startswith('<think>'):
            self.think_depth += 1
            self.in_think_block = True
            logger.debug(f"ğŸ”§ è¿›å…¥thinkå—ï¼Œæ·±åº¦: {self.think_depth}")
            return 7
        
        elif remaining.startswith('</think>'):
            self.think_depth = max(0, self.think_depth - 1)
            self.in_think_block = self.think_depth > 0
            logger.debug(f"ğŸ”§ é€€å‡ºthinkå—ï¼Œæ·±åº¦: {self.think_depth}")
            return 8
        
        return 0
    
    def _can_detect_signal_at(self, pos: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åœ¨æŒ‡å®šä½ç½®æ£€æµ‹ä¿¡å·"""
        return (pos + self.signal_len <= len(self.content_buffer) and 
                not self.in_think_block)
    
    def finalize(self) -> Optional[List[Dict[str, Any]]]:
        """æµç»“æŸæ—¶çš„æœ€ç»ˆå¤„ç†"""
        if self.state == "tool_parsing":
            return parse_function_calls_xml(self.content_buffer, self.trigger_signal)
        return None

# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================
class ToolFunction(BaseModel):
    name: str
    description: Optional[str] = None
    parameters: Dict[str, Any]

class Tool(BaseModel):
    type: Literal["function"]
    function: ToolFunction

class Message(BaseModel):
    role: str
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None
    name: Optional[str] = None
    
    model_config = {"extra": "allow"}

class ToolChoice(BaseModel):
    type: Literal["function"]
    function: Dict[str, str]

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Dict[str, Any]]
    tools: Optional[List[Tool]] = None
    tool_choice: Optional[Union[str, ToolChoice]] = None
    stream: Optional[bool] = False
    stream_options: Optional[Dict[str, Any]] = None
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    n: Optional[int] = None
    stop: Optional[Union[str, List[str]]] = None
    
    model_config = {"extra": "allow"}
    
    @field_validator('messages')
    @classmethod
    def validate_messages(cls, v):
        """éªŒè¯æ¶ˆæ¯æ ¼å¼"""
        if not v:
            raise ValueError("Messages list cannot be empty")
        for msg in v:
            if not isinstance(msg, dict):
                raise ValueError("Each message must be a dictionary")
            if 'role' not in msg:
                raise ValueError("Each message must contain 'role' field")
        return v

def generate_function_prompt(tools: List[Tool], trigger_signal: str) -> tuple[str, str]:
    """
    æ ¹æ®å®¢æˆ·ç«¯è¯·æ±‚ä¸­çš„å·¥å…·å®šä¹‰ç”Ÿæˆæ³¨å…¥çš„ç³»ç»Ÿæç¤ºã€‚
    è¿”å›: (æç¤ºå†…å®¹, è§¦å‘ä¿¡å·)
    """
    tools_list_str = []
    for i, tool in enumerate(tools):
        func = tool.function
        name = func.name
        description = func.description or ""

        # å¼ºå¥åœ°è¯»å–JSON Schemaå­—æ®µ
        schema: Dict[str, Any] = func.parameters or {}
        props: Dict[str, Any] = schema.get("properties", {}) or {}
        required_list: List[str] = schema.get("required", []) or []

        # ç®€çŸ­æ‘˜è¦è¡Œ: name (type)
        params_summary = ", ".join([
            f"{p_name} ({(p_info or {}).get('type', 'any')})" for p_name, p_info in props.items()
        ]) or "None"

        # ä¸ºæç¤ºæ³¨å…¥æ„å»ºè¯¦ç»†å‚æ•°è§„èŒƒï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        detail_lines: List[str] = []
        for p_name, p_info in props.items():
            p_info = p_info or {}
            p_type = p_info.get("type", "any")
            is_required = "Yes" if p_name in required_list else "No"
            p_desc = p_info.get("description")
            enum_vals = p_info.get("enum")
            default_val = p_info.get("default")
            examples_val = p_info.get("examples") or p_info.get("example")

            # å¸¸è§çº¦æŸå’Œæç¤º
            constraints: Dict[str, Any] = {}
            for key in [
                "minimum", "maximum", "exclusiveMinimum", "exclusiveMaximum",
                "minLength", "maxLength", "pattern", "format",
                "minItems", "maxItems", "uniqueItems"
            ]:
                if key in p_info:
                    constraints[key] = p_info.get(key)

            # æ•°ç»„é¡¹ç±»å‹æç¤º
            if p_type == "array":
                items = p_info.get("items") or {}
                if isinstance(items, dict):
                    itype = items.get("type")
                    if itype:
                        constraints["items.type"] = itype

            # ç»„æˆæ¼‚äº®çš„è¡Œ
            detail_lines.append(f"- {p_name}:")
            detail_lines.append(f"  - type: {p_type}")
            detail_lines.append(f"  - required: {is_required}")
            if p_desc:
                detail_lines.append(f"  - description: {p_desc}")
            if enum_vals is not None:
                try:
                    detail_lines.append(f"  - enum: {json.dumps(enum_vals, ensure_ascii=False)}")
                except Exception:
                    detail_lines.append(f"  - enum: {enum_vals}")
            if default_val is not None:
                try:
                    detail_lines.append(f"  - default: {json.dumps(default_val, ensure_ascii=False)}")
                except Exception:
                    detail_lines.append(f"  - default: {default_val}")
            if examples_val is not None:
                try:
                    detail_lines.append(f"  - examples: {json.dumps(examples_val, ensure_ascii=False)}")
                except Exception:
                    detail_lines.append(f"  - examples: {examples_val}")
            if constraints:
                try:
                    detail_lines.append(f"  - constraints: {json.dumps(constraints, ensure_ascii=False)}")
                except Exception:
                    detail_lines.append(f"  - constraints: {constraints}")

        detail_block = "\n".join(detail_lines) if detail_lines else "(no parameter details)"

        desc_block = f"```\n{description}\n```" if description else "None"

        tools_list_str.append(
            f"{i + 1}. <tool name=\"{name}\">\n"
            f"   Description:\n{desc_block}\n"
            f"   Parameters summary: {params_summary}\n"
            f"   Required parameters: {', '.join(required_list) if required_list else 'None'}\n"
            f"   Parameter details:\n{detail_block}"
        )
    
    prompt_template = get_function_call_prompt_template(trigger_signal)
    prompt_content = prompt_template.replace("{tools_list}", "\n\n".join(tools_list_str))
    
    return prompt_content, trigger_signal

def find_upstream(model_name: str) -> tuple[Dict[str, Any], str]:
    """æ ¹æ®æ¨¡å‹åç§°æŸ¥æ‰¾ä¸Šæ¸¸é…ç½®ï¼Œå¤„ç†åˆ«åå’Œç›´é€šæ¨¡å¼ã€‚"""
    
    # å¤„ç†æ¨¡å‹ç›´é€šæ¨¡å¼
    if app_config.features.model_passthrough:
        logger.info("ğŸ”„ æ¨¡å‹ç›´é€šæ¨¡å¼å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚è½¬å‘åˆ°'openai'æœåŠ¡ã€‚")
        openai_service = None
        for service in app_config.upstream_services:
            if service.name == "openai":
                openai_service = service.model_dump()
                break
        
        if openai_service:
            if not openai_service.get("api_key"):
                 raise HTTPException(status_code=500, detail="é…ç½®é”™è¯¯: åœ¨æ¨¡å‹ç›´é€šæ¨¡å¼ä¸‹æœªæ‰¾åˆ°'openai'æœåŠ¡çš„APIå¯†é’¥ã€‚")
            # åœ¨ç›´é€šæ¨¡å¼ä¸‹ï¼Œç›´æ¥ä½¿ç”¨è¯·æ±‚ä¸­çš„æ¨¡å‹åç§°ã€‚
            return openai_service, model_name
        else:
            raise HTTPException(status_code=500, detail="é…ç½®é”™è¯¯: å¯ç”¨äº†'model_passthrough'ï¼Œä½†æœªæ‰¾åˆ°åä¸º'openai'çš„ä¸Šæ¸¸æœåŠ¡ã€‚")

    # é»˜è®¤è·¯ç”±é€»è¾‘
    chosen_model_entry = model_name
    
    if model_name in ALIAS_MAPPING:
        chosen_model_entry = random.choice(ALIAS_MAPPING[model_name])
        logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹åˆ«å'{model_name}'ã€‚ä¸ºæ­¤è¯·æ±‚éšæœºé€‰æ‹©äº†'{chosen_model_entry}'ã€‚")

    service = MODEL_TO_SERVICE_MAPPING.get(chosen_model_entry)
    
    if service:
        if not service.get("api_key"):
            raise HTTPException(status_code=500, detail=f"æ¨¡å‹é…ç½®é”™è¯¯: æœªæ‰¾åˆ°æœåŠ¡'{service.get('name')}'çš„APIå¯†é’¥ã€‚")
    else:
        logger.warning(f"âš ï¸  é…ç½®ä¸­æœªæ‰¾åˆ°æ¨¡å‹'{model_name}'ï¼Œä½¿ç”¨é»˜è®¤æœåŠ¡")
        service = DEFAULT_SERVICE
        if not service.get("api_key"):
            raise HTTPException(status_code=500, detail="æœåŠ¡é…ç½®é”™è¯¯: æœªæ‰¾åˆ°é»˜è®¤APIå¯†é’¥ã€‚")

    actual_model_name = chosen_model_entry
    if ':' in chosen_model_entry:
         parts = chosen_model_entry.split(':', 1)
         if len(parts) == 2:
             _, actual_model_name = parts
            
    return service, actual_model_name

def validate_message_structure(messages: List[Dict[str, Any]]) -> bool:
    """éªŒè¯æ¶ˆæ¯ç»“æ„æ˜¯å¦ç¬¦åˆè¦æ±‚"""
    try:
        valid_roles = ["system", "user", "assistant", "tool"]
        if not app_config.features.convert_developer_to_system:
            valid_roles.append("developer")
        
        for i, msg in enumerate(messages):
            if "role" not in msg:
                logger.error(f"âŒ æ¶ˆæ¯ {i} ç¼ºå°‘roleå­—æ®µ")
                return False
            
            if msg["role"] not in valid_roles:
                logger.error(f"âŒ æ¶ˆæ¯ {i} çš„roleå€¼æ— æ•ˆ: {msg['role']}")
                return False
            
            if msg["role"] == "tool":
                if "tool_call_id" not in msg:
                    logger.error(f"âŒ å·¥å…·æ¶ˆæ¯ {i} ç¼ºå°‘tool_call_idå­—æ®µ")
                    return False
            
            content = msg.get("content")
            content_info = ""
            if content:
                if isinstance(content, str):
                    content_info = f", content=text({len(content)} chars)"
                elif isinstance(content, list):
                    text_parts = [item for item in content if isinstance(item, dict) and item.get('type') == 'text']
                    image_parts = [item for item in content if isinstance(item, dict) and item.get('type') == 'image_url']
                    content_info = f", content=multimodal(text={len(text_parts)}, images={len(image_parts)})"
                else:
                    content_info = f", content={type(content).__name__}"
            else:
                content_info = ", content=empty"
            
            logger.debug(f"âœ… æ¶ˆæ¯ {i} éªŒè¯é€šè¿‡: role={msg['role']}{content_info}")
        
        logger.debug(f"âœ… æ‰€æœ‰æ¶ˆæ¯éªŒè¯æˆåŠŸï¼Œæ€»å…± {len(messages)} æ¡æ¶ˆæ¯")
        return True
    except Exception as e:
        logger.error(f"âŒ æ¶ˆæ¯éªŒè¯å¼‚å¸¸: {e}")
        return False

def safe_process_tool_choice(tool_choice) -> str:
    """å®‰å…¨å¤„ç†tool_choiceå­—æ®µï¼Œé¿å…ç±»å‹é”™è¯¯"""
    try:
        if tool_choice is None:
            return ""
        
        if isinstance(tool_choice, str):
            if tool_choice == "none":
                return "\n\n**é‡è¦:** æ‚¨è¢«ç¦æ­¢åœ¨æœ¬è½®å¯¹è¯ä¸­ä½¿ç”¨ä»»ä½•å·¥å…·ã€‚è¯·åƒæ­£å¸¸çš„èŠå¤©åŠ©æ‰‹ä¸€æ ·å›åº”å¹¶ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            else:
                logger.debug(f"ğŸ”§ æœªçŸ¥çš„tool_choiceå­—ç¬¦ä¸²å€¼: {tool_choice}")
                return ""
        
        elif hasattr(tool_choice, 'function') and hasattr(tool_choice.function, 'name'):
            required_tool_name = tool_choice.function.name
            return f"\n\n**é‡è¦:** åœ¨æœ¬è½®å¯¹è¯ä¸­ï¼Œæ‚¨å¿…é¡»ä»…ä½¿ç”¨åä¸º`{required_tool_name}`çš„å·¥å…·ã€‚ç”Ÿæˆå¿…è¦çš„å‚æ•°å¹¶ä»¥æŒ‡å®šçš„XMLæ ¼å¼è¾“å‡ºã€‚"
        
        else:
            logger.debug(f"ğŸ”§ ä¸æ”¯æŒçš„tool_choiceç±»å‹: {type(tool_choice)}")
            return ""
    
    except Exception as e:
        logger.error(f"âŒ å¤„ç†tool_choiceæ—¶å‡ºé”™: {e}")
        return ""

def preprocess_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """é¢„å¤„ç†æ¶ˆæ¯ï¼Œå°†å·¥å…·ç±»å‹æ¶ˆæ¯è½¬æ¢ä¸ºAIå¯ç†è§£çš„æ ¼å¼ï¼Œè¿”å›å­—å…¸åˆ—è¡¨ä»¥é¿å…PydanticéªŒè¯é—®é¢˜"""
    processed_messages = []
    
    for message in messages:
        if isinstance(message, dict):
            if message.get("role") == "tool":
                tool_call_id = message.get("tool_call_id")
                content = message.get("content")
                
                if tool_call_id and content:
                    formatted_content = format_tool_result_for_ai(tool_call_id, content)
                    processed_message = {
                        "role": "user",
                        "content": formatted_content
                    }
                    processed_messages.append(processed_message)
                    logger.debug(f"ğŸ”§ å°†å·¥å…·æ¶ˆæ¯è½¬æ¢ä¸ºç”¨æˆ·æ¶ˆæ¯: tool_call_id={tool_call_id}")
                else:
                    logger.debug(f"ğŸ”§ è·³è¿‡æ— æ•ˆçš„å·¥å…·æ¶ˆæ¯: tool_call_id={tool_call_id}, content={bool(content)}")
            elif message.get("role") == "assistant" and "tool_calls" in message and message["tool_calls"]:
                tool_calls = message.get("tool_calls", [])
                formatted_tool_calls_str = format_assistant_tool_calls_for_ai(tool_calls, GLOBAL_TRIGGER_SIGNAL)
                
                # ä¸åŸå§‹å†…å®¹ç»„åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                original_content = message.get("content") or ""
                final_content = f"{original_content}\n{formatted_tool_calls_str}".strip()

                processed_message = {
                    "role": "assistant",
                    "content": final_content
                }
                # ä»åŸå§‹æ¶ˆæ¯å¤åˆ¶å…¶ä»–æ½œåœ¨é”®ï¼Œé™¤äº†tool_calls
                for key, value in message.items():
                    if key not in ["role", "content", "tool_calls"]:
                        processed_message[key] = value

                processed_messages.append(processed_message)
                logger.debug(f"ğŸ”§ å°†assistant tool_callsè½¬æ¢ä¸ºcontentã€‚")

            elif message.get("role") == "developer":
                if app_config.features.convert_developer_to_system:
                    processed_message = message.copy()
                    processed_message["role"] = "system"
                    processed_messages.append(processed_message)
                    logger.debug(f"ğŸ”§ å°†developeræ¶ˆæ¯è½¬æ¢ä¸ºsystemæ¶ˆæ¯ï¼Œä»¥ä¾¿æ›´å¥½åœ°ä¸ä¸Šæ¸¸å…¼å®¹")
                else:
                    processed_messages.append(message)
                    logger.debug(f"ğŸ”§ ä¿æŒdeveloperè§’è‰²ä¸å˜ï¼ˆåŸºäºé…ç½®ï¼‰")
            else:
                processed_messages.append(message)
        else:
            processed_messages.append(message)
    
    return processed_messages

# ==================== OpenAI å®¢æˆ·ç«¯å’Œ FastAPI åº”ç”¨ ====================
TOOL_CALL_MAPPING_MANAGER = ToolCallMappingManager(
    max_size=1000,
    ttl_seconds=3600,
    cleanup_interval=300
)

openai_client = None
http_client = httpx.AsyncClient()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global openai_client
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    if 'OPENAI_API_KEY' in globals() and 'OPENAI_BASE_URL' in globals():
        openai_client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        logger.info(f"OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œbase_url: {OPENAI_BASE_URL}")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    if openai_client:
        await openai_client.close()
    await http_client.aclose()
    logger.info("HTTPå’ŒOpenAIå®¢æˆ·ç«¯å·²å…³é—­")

app = FastAPI(
    title="Toolify Token Counter",
    description="é›†æˆäº†å‡½æ•°è°ƒç”¨å¢å¼ºå’ŒTokenè®¡æ•°åŠŸèƒ½çš„ä»£ç†æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# ==================== ä¸­é—´ä»¶å’Œå¼‚å¸¸å¤„ç† ====================
@app.middleware("http")
async def debug_middleware(request: Request, call_next):
    """ç”¨äºè°ƒè¯•éªŒè¯é”™è¯¯çš„ä¸­é—´ä»¶ï¼Œä¸è®°å½•å¯¹è¯å†…å®¹ã€‚"""
    response = await call_next(request)
    
    if response.status_code == 422:
        logger.debug(f"ğŸ” æ£€æµ‹åˆ°{request.method} {request.url.path}çš„éªŒè¯é”™è¯¯")
        logger.debug(f"ğŸ” å“åº”çŠ¶æ€ç : 422 (PydanticéªŒè¯å¤±è´¥)")
    
    return response

@app.exception_handler(ValidationError)
async def validation_exception_handler(request: Request, exc: ValidationError):
    """å¤„ç†PydanticéªŒè¯é”™è¯¯ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯"""
    logger.error(f"âŒ PydanticéªŒè¯é”™è¯¯: {exc}")
    logger.error(f"âŒ è¯·æ±‚URL: {request.url}")
    logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {exc.errors()}")
    
    for error in exc.errors():
        logger.error(f"âŒ éªŒè¯é”™è¯¯ä½ç½®: {error.get('loc')}")
        logger.error(f"âŒ éªŒè¯é”™è¯¯æ¶ˆæ¯: {error.get('msg')}")
        logger.error(f"âŒ éªŒè¯é”™è¯¯ç±»å‹: {error.get('type')}")
    
    return JSONResponse(
        status_code=422,
        content={
            "error": {
                "message": "è¯·æ±‚æ ¼å¼æ— æ•ˆ",
                "type": "invalid_request_error",
                "code": "invalid_request"
            }
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """å¤„ç†æ‰€æœ‰æœªæ•è·çš„å¼‚å¸¸"""
    logger.error(f"âŒ æœªå¤„ç†çš„å¼‚å¸¸: {exc}")
    logger.error(f"âŒ è¯·æ±‚URL: {request.url}")
    logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(exc).__name__}")
    logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "message": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
                "type": "server_error",
                "code": "internal_error"
            }
        }
    )

async def verify_api_key(authorization: str = Header(...)):
    """ä¾èµ–é¡¹: éªŒè¯å®¢æˆ·ç«¯APIå¯†é’¥"""
    client_key = authorization.replace("Bearer ", "")
    if app_config.features.key_passthrough:
        # åœ¨ç›´é€šæ¨¡å¼ä¸‹ï¼Œè·³è¿‡allowed_keysæ£€æŸ¥
        return client_key
    if client_key not in ALLOWED_CLIENT_KEYS:
        raise HTTPException(status_code=401, detail="æœªæˆæƒ")
    return client_key

# ==================== æµå¼å“åº”å¤„ç† ====================
async def stream_response(response, model: str, prompt_tokens: int, start_time: float, request_id: str = None) -> AsyncGenerator[str, None]:
    """å¤„ç†æµå¼å“åº”ï¼ˆé’ˆå¯¹tokenè®¡æ•°ï¼‰- æ”¯æŒOpenAIå¯¹è±¡å’Œå­—ç¬¦ä¸²è¡Œä¸¤ç§è¾“å…¥"""
    completion_tokens = 0
    completion_text = ""
    last_chunk_data = None
    
    try:
        async for chunk in response:
            # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„è¾“å…¥ (æ¥è‡ªhttpx.aiter_lines())
            if isinstance(chunk, str):
                # åªå¤„ç†SSEæ ¼å¼çš„æ•°æ®è¡Œ
                if chunk.startswith("data: "):
                    line_data = chunk[len("data: "):].strip()
                    
                    # å¤„ç†ç»“æŸæ ‡è®°
                    if line_data == "[DONE]":
                        continue
                    
                    # å°è¯•è§£æJSON
                    if line_data:
                        try:
                            chunk_json = json.loads(line_data)
                            
                            # ä»JSONä¸­æå–å†…å®¹
                            if "choices" in chunk_json and len(chunk_json["choices"]) > 0:
                                delta = chunk_json["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    completion_text += content
                            
                            # ä¿å­˜æœ€åä¸€ä¸ªchunkæ•°æ®
                            last_chunk_data = chunk_json
                            
                            # ç›´æ¥ä¼ é€’åŸå§‹SSEæ ¼å¼æ•°æ®
                            yield chunk + "\n\n"
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯
                            if "usage" in chunk_json:
                                completion_tokens = chunk_json["usage"].get("completion_tokens", 0)
                        except json.JSONDecodeError:
                            # æ— æ³•è§£æä¸ºJSONçš„è¡Œï¼ŒåŸæ ·ä¼ é€’
                            yield chunk + "\n\n"
                else:
                    # éSSEæ ¼å¼æ•°æ®ï¼ŒåŸæ ·ä¼ é€’
                    yield chunk + "\n\n"
            
            # å¤„ç†å¯¹è±¡ç±»å‹çš„è¾“å…¥ (æ¥è‡ªOpenAI API)
            elif hasattr(chunk, 'choices') and chunk.choices:
                # æ”¶é›†å®Œæˆæ–‡æœ¬ä»¥è®¡ç®—token
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content:
                    completion_text += delta.content
                
                # ä¿å­˜æœ€åä¸€ä¸ªchunkçš„æ•°æ®ï¼ˆç”¨äºæ„å»ºusage chunkï¼‰
                last_chunk_data = chunk
                
                # è½¬æ¢ä¸º OpenAI æ ¼å¼çš„ SSEï¼Œä½†ä¸å‘é€usageä¿¡æ¯
                chunk_dict = chunk.model_dump()
                # ç§»é™¤å¯èƒ½å­˜åœ¨çš„usageå­—æ®µï¼Œæˆ‘ä»¬ä¼šåœ¨æœ€åæ·»åŠ 
                if 'usage' in chunk_dict:
                    chunk_dict.pop('usage')
                yield f"data: {json.dumps(chunk_dict)}\n\n"
                
                # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯ï¼ˆOpenAIåœ¨æœ€åä¸€ä¸ªchunkä¸­æä¾›ï¼‰
                if hasattr(chunk, 'usage') and chunk.usage:
                    completion_tokens = chunk.usage.completion_tokens
        
        # å¦‚æœæ²¡æœ‰ä»APIè·å–åˆ°completion_tokensï¼Œåˆ™æ‰‹åŠ¨è®¡ç®—
        if completion_tokens == 0 and completion_text:
            completion_tokens = token_counter.count_text_tokens(completion_text, model)
        
        total_tokens = prompt_tokens + completion_tokens
        elapsed_time = time.time() - start_time
        
        # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡ - æ¨¡å‹: {model}")
        logger.info(f"   è¾“å…¥ Tokens: {prompt_tokens}")
        logger.info(f"   è¾“å‡º Tokens: {completion_tokens}")
        logger.info(f"   æ€»è®¡ Tokens: {total_tokens}")
        logger.info(f"   è€—æ—¶: {elapsed_time:.2f}ç§’")
        logger.info("=" * 60)
        
        # å‘é€åŒ…å«ä»£ç†è®¡ç®—çš„usageä¿¡æ¯çš„chunkï¼ˆç¬¦åˆOpenAIæ ¼å¼ï¼‰
        if last_chunk_data:
            usage_chunk = {
                "id": last_chunk_data.id if hasattr(last_chunk_data, 'id') else f"chatcmpl-{int(time.time())}",
                "object": "chat.completion.chunk",
                "created": last_chunk_data.created if hasattr(last_chunk_data, 'created') else int(time.time()),
                "model": model,
                "choices": [],
                "usage": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens
                }
            }
            yield f"data: {json.dumps(usage_chunk)}\n\n"
        
        yield "data: [DONE]\n\n"
    except Exception as e:
        logger.error(f"æµé”™è¯¯: {e}")
        yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"

async def stream_proxy_with_fc_transform(url: str, body: dict, headers: dict, model: str, has_fc: bool, trigger_signal: str):
    """
    å¢å¼ºå‹æµå¼ä»£ç†ï¼Œæ”¯æŒåŠ¨æ€è§¦å‘ä¿¡å·ï¼Œé¿å…åœ¨thinkæ ‡ç­¾å†…è¯¯åˆ¤
    """
    logger.info(f"ğŸ“ å¼€å§‹æ¥è‡ª {url} çš„æµå¼å“åº”")
    logger.info(f"ğŸ“ å‡½æ•°è°ƒç”¨å·²å¯ç”¨: {has_fc}")

    if not has_fc or not trigger_signal:
        try:
            async with http_client.stream("POST", url, json=body, headers=headers, timeout=app_config.server.timeout) as response:
                async for chunk in response.aiter_bytes():
                    yield chunk
        except httpx.RemoteProtocolError:
            logger.debug("ğŸ”§ ä¸Šæ¸¸è¿‡æ—©å…³é—­è¿æ¥ï¼Œç»“æŸæµå¼å“åº”")
            return
        return

    detector = StreamingFunctionCallDetector(trigger_signal)

    def _prepare_tool_calls(parsed_tools: List[Dict[str, Any]]):
        tool_calls = []
        for i, tool in enumerate(parsed_tools):
            tool_call_id = f"call_{uuid.uuid4().hex}"
            store_tool_call_mapping(
                tool_call_id,
                tool["name"],
                tool["args"],
                f"è°ƒç”¨å·¥å…· {tool['name']}"
            )
            tool_calls.append({
                "index": i, "id": tool_call_id, "type": "function",
                "function": { "name": tool["name"], "arguments": json.dumps(tool["args"]) }
            })
        return tool_calls

    def _build_tool_call_sse_chunks(parsed_tools: List[Dict[str, Any]], model_id: str) -> List[str]:
        tool_calls = _prepare_tool_calls(parsed_tools)
        chunks: List[str] = []

        initial_chunk = {
            "id": f"chatcmpl-{uuid.uuid4().hex}", "object": "chat.completion.chunk",
            "created": int(os.path.getmtime(__file__)), "model": model_id,
            "choices": [{"index": 0, "delta": {"role": "assistant", "content": None, "tool_calls": tool_calls}, "finish_reason": None}],
        }
        chunks.append(f"data: {json.dumps(initial_chunk)}\n\n")


        final_chunk = {
             "id": f"chatcmpl-{uuid.uuid4().hex}", "object": "chat.completion.chunk",
            "created": int(os.path.getmtime(__file__)), "model": model_id,
            "choices": [{"index": 0, "delta": {}, "finish_reason": "tool_calls"}],
        }
        chunks.append(f"data: {json.dumps(final_chunk)}\n\n")
        chunks.append("data: [DONE]\n\n")
        return chunks

    try:
        async with http_client.stream("POST", url, json=body, headers=headers, timeout=app_config.server.timeout) as response:
            if response.status_code != 200:
                error_content = await response.aread()
                logger.error(f"âŒ ä¸Šæ¸¸æœåŠ¡æµå“åº”é”™è¯¯: status_code={response.status_code}")
                logger.error(f"âŒ ä¸Šæ¸¸é”™è¯¯è¯¦æƒ…: {error_content.decode('utf-8', errors='ignore')}")
                
                if response.status_code == 401:
                    error_message = "èº«ä»½éªŒè¯å¤±è´¥"
                elif response.status_code == 403:
                    error_message = "æ‹’ç»è®¿é—®"
                elif response.status_code == 429:
                    error_message = "è¶…å‡ºé€Ÿç‡é™åˆ¶"
                elif response.status_code >= 500:
                    error_message = "ä¸Šæ¸¸æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
                else:
                    error_message = "è¯·æ±‚å¤„ç†å¤±è´¥"
                
                error_chunk = {"error": {"message": error_message, "type": "upstream_error"}}
                yield f"data: {json.dumps(error_chunk)}\n\n"
                yield "data: [DONE]\n\n"
                return

            async for line in response.aiter_lines():
                if detector.state == "tool_parsing":
                    if line.startswith("data:"):
                        line_data = line[len("data: "):].strip()
                        if line_data and line_data != "[DONE]":
                            try:
                                chunk_json = json.loads(line_data)
                                delta_content = chunk_json.get("choices", [{}])[0].get("delta", {}).get("content", "") or ""
                                detector.content_buffer += delta_content
                                # æå‰ç»ˆæ­¢ï¼šä¸€æ—¦å‡ºç°</function_calls>ï¼Œç«‹å³è§£æå¹¶å®Œæˆ
                                if "</function_calls>" in detector.content_buffer:
                                    logger.debug("ğŸ”§ åœ¨æµä¸­æ£€æµ‹åˆ°</function_calls>ï¼Œæå‰ç»“æŸ...")
                                    parsed_tools = detector.finalize()
                                    if parsed_tools:
                                        logger.debug(f"ğŸ”§ æå‰å®Œæˆ: è§£æäº† {len(parsed_tools)} ä¸ªå·¥å…·è°ƒç”¨")
                                        for sse in _build_tool_call_sse_chunks(parsed_tools, model):
                                            yield sse
                                        return
                                    else:
                                        logger.error("âŒ æå‰å®Œæˆè§£æå·¥å…·è°ƒç”¨å¤±è´¥")
                                        error_content = "é”™è¯¯: æ£€æµ‹åˆ°å·¥å…·ä½¿ç”¨ä¿¡å·ä½†æœªèƒ½è§£æå‡½æ•°è°ƒç”¨æ ¼å¼"
                                        error_chunk = { "id": "error-chunk", "choices": [{"delta": {"content": error_content}}]}
                                        yield f"data: {json.dumps(error_chunk)}\n\n"
                                        yield "data: [DONE]\n\n"
                                        return
                            except (json.JSONDecodeError, IndexError):
                                pass
                    continue
                
                if line.startswith("data:"):
                    line_data = line[len("data: "):].strip()
                    if not line_data or line_data == "[DONE]":
                        continue
                    
                    try:
                        chunk_json = json.loads(line_data)
                        delta_content = chunk_json.get("choices", [{}])[0].get("delta", {}).get("content", "") or ""
                        
                        if delta_content:
                            is_detected, content_to_yield = detector.process_chunk(delta_content)
                            
                            if content_to_yield:
                                yield_chunk = {
                                    "id": f"chatcmpl-passthrough-{uuid.uuid4().hex}",
                                    "object": "chat.completion.chunk",
                                    "created": int(os.path.getmtime(__file__)),
                                    "model": model,
                                    "choices": [{"index": 0, "delta": {"content": content_to_yield}}]
                                }
                                yield f"data: {json.dumps(yield_chunk)}\n\n"
                            
                            if is_detected:
                                # æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ä¿¡å·ï¼Œåˆ‡æ¢åˆ°è§£ææ¨¡å¼
                                continue
                    
                    except (json.JSONDecodeError, IndexError):
                        yield line + "\n\n"

    except httpx.RequestError as e:
        logger.error(f"âŒ è¿æ¥åˆ°ä¸Šæ¸¸æœåŠ¡å¤±è´¥: {e}")
        logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        
        error_message = "è¿æ¥åˆ°ä¸Šæ¸¸æœåŠ¡å¤±è´¥"
        error_chunk = {"error": {"message": error_message, "type": "connection_error"}}
        yield f"data: {json.dumps(error_chunk)}\n\n"
        yield "data: [DONE]\n\n"
        return

    if detector.state == "tool_parsing":
        logger.debug(f"ğŸ”§ æµç»“æŸï¼Œå¼€å§‹è§£æå·¥å…·è°ƒç”¨XML...")
        parsed_tools = detector.finalize()
        if parsed_tools:
            logger.debug(f"ğŸ”§ æµå¤„ç†ï¼šæˆåŠŸè§£æäº† {len(parsed_tools)} ä¸ªå·¥å…·è°ƒç”¨")
            for sse in _build_tool_call_sse_chunks(parsed_tools, model):
                yield sse
            return
        else:
            logger.error(f"âŒ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ä¿¡å·ä½†XMLè§£æå¤±è´¥ï¼Œç¼“å†²åŒºå†…å®¹: {detector.content_buffer}")
            error_content = "é”™è¯¯: æ£€æµ‹åˆ°å·¥å…·ä½¿ç”¨ä¿¡å·ä½†æœªèƒ½è§£æå‡½æ•°è°ƒç”¨æ ¼å¼"
            error_chunk = { "id": "error-chunk", "choices": [{"delta": {"content": error_content}}]}
            yield f"data: {json.dumps(error_chunk)}\n\n"

    elif detector.state == "detecting" and detector.content_buffer:
        # å¦‚æœæµå·²ç»“æŸä½†ç¼“å†²åŒºä»æœ‰å‰©ä½™å­—ç¬¦ä¸è¶³ä»¥å½¢æˆä¿¡å·ï¼Œè¾“å‡ºå®ƒä»¬
        final_yield_chunk = {
            "id": f"chatcmpl-finalflush-{uuid.uuid4().hex}", "object": "chat.completion.chunk",
            "created": int(os.path.getmtime(__file__)), "model": model,
            "choices": [{"index": 0, "delta": {"content": detector.content_buffer}}]
        }
        yield f"data: {json.dumps(final_yield_chunk)}\n\n"

    yield "data: [DONE]\n\n"

# ==================== API ç«¯ç‚¹ ====================
@app.get("/")
def read_root():
    """å¥åº·æ£€æŸ¥"""
    features_info = {}
    try:
        features_info = {
            "function_calling": app_config.features.enable_function_calling,
            "log_level": app_config.features.log_level,
            "convert_developer_to_system": app_config.features.convert_developer_to_system,
            "random_trigger": True,
            "token_counting": True
        }
    except:
        features_info = {
            "token_counting": True
        }
    
    return {
        "status": "running",
        "service": "Toolify Token Counter",
        "version": "1.0.0",
        "features": features_info
    }

@app.get("/v1/models")
async def list_models(_api_key: str = Depends(verify_api_key)):
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    try:
        if openai_client:
            # å¦‚æœæ˜¯ä½œä¸ºtokenè®¡æ•°ä»£ç†ä½¿ç”¨ï¼Œä»OpenAIè·å–æ¨¡å‹
            models = await openai_client.models.list()
            return models.model_dump()
        else:
            # å¦‚æœæ˜¯ä½œä¸ºå‡½æ•°è°ƒç”¨ä»£ç†ä½¿ç”¨ï¼Œä½¿ç”¨é…ç½®çš„æ¨¡å‹
            visible_models = set()
            for model_name in MODEL_TO_SERVICE_MAPPING.keys():
                if ':' in model_name:
                    parts = model_name.split(':', 1)
                    if len(parts) == 2:
                        alias, _ = parts
                        visible_models.add(alias)
                    else:
                        visible_models.add(model_name)
                else:
                    visible_models.add(model_name)

            models = []
            for model_id in sorted(visible_models):
                models.append({
                    "id": model_id,
                    "object": "model",
                    "created": 1677610602,
                    "owned_by": "openai",
                    "permission": [],
                    "root": model_id,
                    "parent": None
                })
            
            return {
                "object": "list",
                "data": models
            }
    except Exception as e:
        logger.error(f"åˆ—å‡ºæ¨¡å‹æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/v1/usage/tokens")
async def get_token_count(text: str, model: str = "gpt-5-high"):
    """è·å–æ–‡æœ¬çš„ token æ•°é‡ï¼ˆè°ƒè¯•æ¥å£ï¼‰"""
    try:
        tokens = token_counter.count_text_tokens(text, model)
        return {
            "text": text,
            "model": model,
            "tokens": tokens
        }
    except Exception as e:
        logger.error(f"è®¡ç®—tokenæ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    _api_key: str = Depends(verify_api_key)
):
    """èŠå¤©è¡¥å…¨æ¥å£ï¼Œæ”¯æŒå‡½æ•°è°ƒç”¨å’Œtokenè®¡æ•°"""
    start_time = time.time()
    
    # è®¡ç®—è¾“å…¥token
    prompt_tokens = token_counter.count_tokens(request.messages, request.model)
    logger.info(f"è¯·æ±‚åˆ° {request.model} - è¾“å…¥tokens: {prompt_tokens}")
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å‡½æ•°è°ƒç”¨ä»£ç†
        if 'MODEL_TO_SERVICE_MAPPING' in globals() and 'GLOBAL_TRIGGER_SIGNAL' in globals():
            logger.debug(f"ğŸ”§ æ”¶åˆ°è¯·æ±‚ï¼Œæ¨¡å‹: {request.model}")
            logger.debug(f"ğŸ”§ æ¶ˆæ¯æ•°é‡: {len(request.messages)}")
            logger.debug(f"ğŸ”§ å·¥å…·æ•°é‡: {len(request.tools) if request.tools else 0}")
            logger.debug(f"ğŸ”§ æµå¼: {request.stream}")
            
            upstream, actual_model = find_upstream(request.model)
            upstream_url = f"{upstream['base_url']}/chat/completions"
            
            logger.debug(f"ğŸ”§ å¼€å§‹æ¶ˆæ¯é¢„å¤„ç†ï¼ŒåŸå§‹æ¶ˆæ¯æ•°é‡: {len(request.messages)}")
            processed_messages = preprocess_messages(request.messages)
            logger.debug(f"ğŸ”§ é¢„å¤„ç†å®Œæˆï¼Œå¤„ç†åæ¶ˆæ¯æ•°é‡: {len(processed_messages)}")
            
            if not validate_message_structure(processed_messages):
                logger.error(f"âŒ æ¶ˆæ¯ç»“æ„éªŒè¯å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†")
            
            request_body_dict = request.model_dump(exclude_unset=True)
            request_body_dict["model"] = actual_model
            request_body_dict["messages"] = processed_messages
            is_fc_enabled = app_config.features.enable_function_calling
            has_tools_in_request = bool(request.tools)
            has_function_call = is_fc_enabled and has_tools_in_request
            
            logger.debug(f"ğŸ”§ è¯·æ±‚ä½“æ„å»ºå®Œæˆï¼Œæ¶ˆæ¯æ•°é‡: {len(processed_messages)}")
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {_api_key}" if app_config.features.key_passthrough else f"Bearer {upstream['api_key']}",
                "Accept": "application/json" if not request.stream else "text/event-stream"
            }

            logger.info(f"ğŸ“ è½¬å‘è¯·æ±‚åˆ°ä¸Šæ¸¸: {upstream['name']}")
            logger.info(f"ğŸ“ æ¨¡å‹: {request_body_dict.get('model', 'unknown')}, æ¶ˆæ¯: {len(request_body_dict.get('messages', []))}")

            if has_function_call:
                logger.debug(f"ğŸ”§ ä½¿ç”¨æ­¤è¯·æ±‚çš„å…¨å±€è§¦å‘ä¿¡å·: {GLOBAL_TRIGGER_SIGNAL}")
                
                function_prompt, _ = generate_function_prompt(request.tools, GLOBAL_TRIGGER_SIGNAL)
                
                tool_choice_prompt = safe_process_tool_choice(request.tool_choice)
                if tool_choice_prompt:
                    function_prompt += tool_choice_prompt

                system_message = {"role": "system", "content": function_prompt}
                request_body_dict["messages"].insert(0, system_message)
                
                if "tools" in request_body_dict:
                    del request_body_dict["tools"]
                if "tool_choice" in request_body_dict:
                    del request_body_dict["tool_choice"]

            elif has_tools_in_request and not is_fc_enabled:
                logger.info(f"ğŸ”§ é…ç½®å·²ç¦ç”¨å‡½æ•°è°ƒç”¨ï¼Œå¿½ç•¥è¯·æ±‚ä¸­çš„'tools'å’Œ'tool_choice'ã€‚")
                if "tools" in request_body_dict:
                    del request_body_dict["tools"]
                if "tool_choice" in request_body_dict:
                    del request_body_dict["tool_choice"]

            if not request.stream:
                try:
                    logger.debug(f"ğŸ”§ å‘é€ä¸Šæ¸¸è¯·æ±‚åˆ°: {upstream_url}")
                    logger.debug(f"ğŸ”§ has_function_call: {has_function_call}")
                    logger.debug(f"ğŸ”§ è¯·æ±‚ä½“åŒ…å«tools: {bool(request.tools)}")
                    
                    upstream_response = await http_client.post(
                        upstream_url, json=request_body_dict, headers=headers, timeout=app_config.server.timeout
                    )
                    upstream_response.raise_for_status() # å¦‚æœçŠ¶æ€ç ä¸º4xxæˆ–5xxï¼Œåˆ™å¼•å‘å¼‚å¸¸
                    
                    response_json = upstream_response.json()
                    logger.debug(f"ğŸ”§ ä¸Šæ¸¸å“åº”çŠ¶æ€ç : {upstream_response.status_code}")
                    
                    # è®¡ç®—è¾“å‡ºtokenå¹¶æ·»åŠ tokenç»Ÿè®¡
                    completion_text = ""
                    if response_json.get("choices") and len(response_json["choices"]) > 0:
                        content = response_json["choices"][0].get("message", {}).get("content")
                        if content:
                            completion_text = content
                    
                    completion_tokens = token_counter.count_text_tokens(completion_text, request.model) if completion_text else 0
                    total_tokens = prompt_tokens + completion_tokens
                    elapsed_time = time.time() - start_time
                    
                    # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯
                    logger.info("=" * 60)
                    logger.info(f"ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡ - æ¨¡å‹: {request.model}")
                    logger.info(f"   è¾“å…¥ Tokens: {prompt_tokens}")
                    logger.info(f"   è¾“å‡º Tokens: {completion_tokens}")
                    logger.info(f"   æ€»è®¡ Tokens: {total_tokens}")
                    logger.info(f"   è€—æ—¶: {elapsed_time:.2f}ç§’")
                    logger.info("=" * 60)
                    
                    # æ·»åŠ tokenç»Ÿè®¡åˆ°å“åº”
                    response_json["usage"] = {
                        "prompt_tokens": prompt_tokens,
                        "completion_tokens": completion_tokens,
                        "total_tokens": total_tokens
                    }
                    
                    if has_function_call:
                        content = response_json["choices"][0]["message"]["content"]
                        logger.debug(f"ğŸ”§ å®Œæ•´å“åº”å†…å®¹: {repr(content)}")
                        
                        parsed_tools = parse_function_calls_xml(content, GLOBAL_TRIGGER_SIGNAL)
                        logger.debug(f"ğŸ”§ XMLè§£æç»“æœ: {parsed_tools}")
                        
                        if parsed_tools:
                            logger.debug(f"ğŸ”§ æˆåŠŸè§£æ {len(parsed_tools)} ä¸ªå·¥å…·è°ƒç”¨")
                            tool_calls = []
                            for tool in parsed_tools:
                                tool_call_id = f"call_{uuid.uuid4().hex}"
                                store_tool_call_mapping(
                                    tool_call_id,
                                    tool["name"],
                                    tool["args"],
                                    f"è°ƒç”¨å·¥å…· {tool['name']}"
                                )
                                tool_calls.append({
                                    "id": tool_call_id,
                                    "type": "function",
                                    "function": {
                                        "name": tool["name"],
                                        "arguments": json.dumps(tool["args"])
                                    }
                                })
                            logger.debug(f"ğŸ”§ è½¬æ¢åçš„tool_calls: {tool_calls}")
                            
                            response_json["choices"][0]["message"] = {
                                "role": "assistant",
                                "content": None,
                                "tool_calls": tool_calls,
                            }
                            response_json["choices"][0]["finish_reason"] = "tool_calls"
                            logger.debug(f"ğŸ”§ å‡½æ•°è°ƒç”¨è½¬æ¢å®Œæˆ")
                        else:
                            logger.debug(f"ğŸ”§ æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œè¿”å›åŸå§‹å†…å®¹ï¼ˆåŒ…æ‹¬thinkå—ï¼‰")
                    else:
                        logger.debug(f"ğŸ”§ æœªæ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨æˆ–è½¬æ¢æ¡ä»¶ä¸æ»¡è¶³")
                    
                    return JSONResponse(content=response_json)

                except httpx.HTTPStatusError as e:
                    logger.error(f"âŒ ä¸Šæ¸¸æœåŠ¡å“åº”é”™è¯¯: status_code={e.response.status_code}")
                    logger.error(f"âŒ ä¸Šæ¸¸é”™è¯¯è¯¦æƒ…: {e.response.text}")
                    
                    if e.response.status_code == 400:
                        error_response = {
                            "error": {
                                "message": "è¯·æ±‚å‚æ•°æ— æ•ˆ",
                                "type": "invalid_request_error",
                                "code": "bad_request"
                            }
                        }
                    elif e.response.status_code == 401:
                        error_response = {
                            "error": {
                                "message": "èº«ä»½éªŒè¯å¤±è´¥",
                                "type": "authentication_error", 
                                "code": "unauthorized"
                            }
                        }
                    elif e.response.status_code == 403:
                        error_response = {
                            "error": {
                                "message": "æ‹’ç»è®¿é—®",
                                "type": "permission_error",
                                "code": "forbidden"
                            }
                        }
                    elif e.response.status_code == 429:
                        error_response = {
                            "error": {
                                "message": "è¶…å‡ºé€Ÿç‡é™åˆ¶",
                                "type": "rate_limit_error",
                                "code": "rate_limit_exceeded"
                            }
                        }
                    elif e.response.status_code >= 500:
                        error_response = {
                            "error": {
                                "message": "ä¸Šæ¸¸æœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
                                "type": "service_error",
                                "code": "upstream_error"
                            }
                        }
                    else:
                        error_response = {
                            "error": {
                                "message": "è¯·æ±‚å¤„ç†å¤±è´¥",
                                "type": "api_error",
                                "code": "unknown_error"
                            }
                        }
                    
                    return JSONResponse(content=error_response, status_code=e.response.status_code)
                
            else:
                if has_function_call:
                    return StreamingResponse(
                        stream_proxy_with_fc_transform(upstream_url, request_body_dict, headers, request.model, has_function_call, GLOBAL_TRIGGER_SIGNAL),
                        media_type="text/event-stream"
                    )
                else:
                    # ä½¿ç”¨tokenè®¡æ•°æµå¼å¤„ç†
                    response = await http_client.post(
                        upstream_url, json=request_body_dict, headers=headers, timeout=app_config.server.timeout
                    )
                    response.raise_for_status()
                    
                    # å¤„ç†æµå¼å“åº” - ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²è¡Œ
                    return StreamingResponse(
                        stream_response(response.aiter_lines(), request.model, prompt_tokens, start_time),
                        media_type="text/event-stream"
                    )
        
        # å¦‚æœä¸æ˜¯å‡½æ•°è°ƒç”¨ä»£ç†ï¼Œåˆ™ä½œä¸ºtokenè®¡æ•°ä»£ç†ä½¿ç”¨
        elif openai_client:
            # ç›´æ¥ä½¿ç”¨ messagesï¼Œå·²ç»æ˜¯å­—å…¸åˆ—è¡¨æ ¼å¼
            messages = request.messages
            
            # è®°å½•é¦–æ¡æ¶ˆæ¯å†…å®¹ç±»å‹ï¼Œä¾¿äºè°ƒè¯•
            if messages and len(messages) > 0:
                first_msg = messages[0]
                content = first_msg.get("content", "")
                content_type = type(content).__name__
                logger.debug(f"First message content type: {content_type}")
                if isinstance(content, list):
                    logger.debug(f"First message content items: {len(content)}")
            
            # è°ƒç”¨ OpenAI API
            response = await openai_client.chat.completions.create(
                model=request.model,
                messages=messages,
                temperature=request.temperature,
                top_p=request.top_p,
                n=request.n,
                stream=request.stream,
                stop=request.stop,
                max_tokens=request.max_tokens,
                presence_penalty=request.presence_penalty,
                frequency_penalty=request.frequency_penalty,
                user=request.user
            )
            
            # æµå¼å“åº”
            if request.stream:
                logger.info(f"å¼€å§‹æµå¼å“åº” - æ¨¡å‹: {request.model}, è¾“å…¥Tokens: {prompt_tokens}")
                return StreamingResponse(
                    stream_response(response, request.model, prompt_tokens, start_time),
                    media_type="text/event-stream"
                )
            
            # éæµå¼å“åº”
            # è®¡ç®—è¾“å‡ºtokenï¼ˆå¦‚æœå“åº”ä¸­æœ‰å†…å®¹çš„è¯ï¼‰
            completion_text = ""
            if response.choices and len(response.choices) > 0:
                choice = response.choices[0]
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                    completion_text = choice.message.content or ""
            
            # ä½¿ç”¨ä»£ç†è®¡ç®—çš„tokenæ•°
            completion_tokens = token_counter.count_text_tokens(completion_text, request.model) if completion_text else 0
            total_tokens = prompt_tokens + completion_tokens
            
            elapsed_time = time.time() - start_time
            
            # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡ - æ¨¡å‹: {request.model}")
            logger.info(f"   è¾“å…¥ Tokens: {prompt_tokens}")
            logger.info(f"   è¾“å‡º Tokens: {completion_tokens}")
            logger.info(f"   æ€»è®¡ Tokens: {total_tokens}")
            logger.info(f"   è€—æ—¶: {elapsed_time:.2f}ç§’")
            logger.info("=" * 60)
            
            # è¿”å›å“åº”ï¼Œä½¿ç”¨ä»£ç†è®¡ç®—çš„usage
            response_dict = response.model_dump()
            response_dict['usage'] = {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens
            }
            return response_dict
            
    except Exception as e:
        logger.error(f"chat_completionsä¸­çš„é”™è¯¯: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/completions")
async def completions(request: Request):
    """æ–‡æœ¬è¡¥å…¨æ¥å£ï¼ˆä»…Tokenè®¡æ•°åŠŸèƒ½ï¼‰"""
    try:
        body = await request.json()
        prompt = body.get("prompt", "")
        model = body.get("model", "gpt-5-high-instruct")
        
        # è®¡ç®—è¾“å…¥ token
        start_time = time.time()
        prompt_tokens = token_counter.count_text_tokens(prompt, model)
        
        # è°ƒç”¨ OpenAI API
        response = await openai_client.completions.create(**body)
        
        # è®¡ç®—è¾“å‡ºtoken
        completion_text = ""
        if response.choices and len(response.choices) > 0:
            choice = response.choices[0]
            if hasattr(choice, 'text'):
                completion_text = choice.text or ""
        
        # ä½¿ç”¨ä»£ç†è®¡ç®—çš„tokenæ•°
        completion_tokens = token_counter.count_text_tokens(completion_text, model) if completion_text else 0
        total_tokens = prompt_tokens + completion_tokens
        elapsed_time = time.time() - start_time
        
        # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡ - æ¨¡å‹: {model}")
        logger.info(f"   è¾“å…¥ Tokens: {prompt_tokens}")
        logger.info(f"   è¾“å‡º Tokens: {completion_tokens}")
        logger.info(f"   æ€»è®¡ Tokens: {total_tokens}")
        logger.info(f"   è€—æ—¶: {elapsed_time:.2f}ç§’")
        logger.info("=" * 60)
        
        # è¿”å›å“åº”ï¼Œä½¿ç”¨ä»£ç†è®¡ç®—çš„usage
        response_dict = response.model_dump()
        response_dict['usage'] = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens
        }
        return response_dict
        
    except Exception as e:
        logger.error(f"Error in completions: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== å¯åŠ¨æœåŠ¡ ====================
if __name__ == "__main__":
    port = PROXY_PORT if 'PROXY_PORT' in globals() else 5115  # ä½¿ç”¨è‡ªå®šä¹‰ç«¯å£5115
    host = PROXY_HOST if 'PROXY_HOST' in globals() else app_config.server.host
    log_level = "info" if 'PROXY_PORT' in globals() else app_config.features.log_level.lower()
    
    if log_level == "disabled":
        log_level = "critical"
    
    logger.info(f"ğŸš€ åœ¨ {host}:{port} å¯åŠ¨æœåŠ¡")
    
    if 'app_config' in globals():
        logger.info(f"â±ï¸ è¯·æ±‚è¶…æ—¶: {app_config.server.timeout} ç§’")
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level=log_level
    )